{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8de43fec-d593-41a1-857f-d80cec8471c8",
   "metadata": {},
   "source": [
    "# Fake News Detector\n",
    "This is a very naive and simple approach which barely registers above random accuracy. You can definitely do better! But it should give you at least a first framework with which to begin. Here are some things to try:\n",
    "\n",
    "## Feature Engineering:\n",
    "Extract additional features from the text, such as:\n",
    "- Number of words in the article.\n",
    "- Number of sentences in the article.\n",
    "- Average word length.\n",
    "- Presence of specific keywords or phrases.\n",
    "- Punctuation counts.\n",
    "- Capitalization features (e.g., ratio of capitalized words).\n",
    "\n",
    "## TF-IDF Vectorization:\n",
    "Instead of using a simple CountVectorizer, try using the TF-IDF vectorization, which takes into account the importance of terms in the entire corpus.\n",
    "    \n",
    "## Word Embeddings:\n",
    "Use pre-trained word embeddings like Word2Vec, GloVe, or fastText to capture semantic relationships between words. This can provide a richer representation of your text compared to a simple bag-of-words model.\n",
    "\n",
    "## Deep Learning:\n",
    "Implement a deep neural network, perhaps a recurrent neural network (RNN) or long short-term memory (LSTM) network, to capture sequential dependencies in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ea1890-19c4-438a-bbc7-1fa6bfcaa723",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2db0f3d-c46c-4781-9b28-bd3b04c66761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(\"fake_news.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6c0329-40d1-4848-a792-07ce69b28f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_genuine = df.copy()\n",
    "df_genuine.drop('fake_news_article', axis=1, inplace=True)  # Drop the 'fake_news_article' column\n",
    "df_genuine.rename(columns={'content': 'article'}, inplace=True)  # Rename 'content' to 'article'\n",
    "df_genuine['is_fake'] = 0  # Set 'is_fake' to 0 for genuine news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0ab2b8-5da4-4ae6-b354-7b7a6a354fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with fake news\n",
    "df_fake = df.copy()\n",
    "df_fake.drop('content', axis=1, inplace=True)  # Drop the 'content' column\n",
    "df_fake.rename(columns={'fake_news_article': 'article'}, inplace=True)  # Rename 'fake_news_article' to 'article'\n",
    "df_fake['is_fake'] = 1  # Set 'is_fake' to 1 for fake news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5af50a-3e3a-4967-a2cc-59c13f8ba047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the two dataframes\n",
    "df_combined = pd.concat([df_genuine, df_fake], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0baedeec-d5b8-4b7a-8deb-e9568104ecd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (X) and target (y)\n",
    "X = df_combined['article']\n",
    "y = df_combined['is_fake']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8fc65e-71f8-4e0c-a5a3-8b9ab10d2a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets while maintaining equal class sizes\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6773d1f-c76c-42f0-8e23-f63a11bfa67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CountVectorizer to convert a collection of text documents to a matrix of token counts\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_counts = vectorizer.fit_transform(X_train)\n",
    "X_test_counts = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d74d90-5dbc-4778-9b59-d13f47ce7750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Multinomial Naive Bayes classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_counts, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dba1620-8f80-40d3-b0ad-c00bb0f3275f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(X_test_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e39176-b243-4952-86f7-91673aa8432e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13bae5d-d3e6-4c32-8bab-fe1688c3ac87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the results\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_rep)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
