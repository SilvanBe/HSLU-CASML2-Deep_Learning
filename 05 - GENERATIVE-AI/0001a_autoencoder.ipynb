{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAHyZgUGrjrS"
      },
      "source": [
        "# Your First Autoencoder\n",
        "\n",
        "Version 1.31\n",
        "\n",
        "(C) 2020 - Umberto Michelucci, Michela Sperti\n",
        "\n",
        "This notebook is part of the book _Applied Deep Learning: a case based approach, **2nd edition**_ from APRESS by [U. Michelucci](mailto:umberto.michelucci@toelt.ai) and [M. Sperti](mailto:michela.sperti@toelt.ai)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OqaeuCam0Bl"
      },
      "source": [
        "The purpose of this notebook is to show you what an autoencoder is and what kind of tasks it can solve, through a real case example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoofZNYbrnyI"
      },
      "source": [
        "## Notebook Learning Goals\n",
        "\n",
        "At the end of this notebook you will be able to build a simple autoencoder with Keras, using `Dense` layers in Keras and apply it to images, in particular to the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset and the [fashion MNIST](https://keras.io/api/datasets/fashion_mnist/) dataset as examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htiQagzW5bb7"
      },
      "source": [
        "## Libraries Import\n",
        "\n",
        "This section contains the necessary libraries (such as tensorflow or pandas) you need to import to run the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUNNih8SlLaO"
      },
      "source": [
        "# general libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import sys\n",
        "\n",
        "from matplotlib import colormaps\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# tensorflow libraries\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow as tf\n",
        "\n",
        "# sklearn libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AvHc2f6GbxR"
      },
      "source": [
        "Now we clone the repository for the book to be able to access the modules that we have written for all the jupyter notebooks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Io-Dq8hTGdiQ"
      },
      "source": [
        "# Referring to the following cell, if you want to re-clone a repository\n",
        "# inside the google colab instance, you need to delete it first.\n",
        "# You can delete the repositories contained in this instance executing\n",
        "# the following two lines of code (deleting the # comment symbol).\n",
        "\n",
        "# !rm -rf ADL-Book-2nd-Ed\n",
        "# !rm -rf BCCD_Dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OqIWhSXGgPU"
      },
      "source": [
        "# This command actually clone the repository of the book in the google colab\n",
        "# instance. In this way this notebook will have access to the modules\n",
        "# we have written for this book.\n",
        "\n",
        "# Please note that in case you have already run this cell, and you run it again\n",
        "# you may get the error message:\n",
        "#\n",
        "# fatal: destination path 'ADL-Book-2nd-Ed' already exists and is not an empty directory.\n",
        "#\n",
        "# In this case you can safely ignore the error message.\n",
        "\n",
        "!git clone https://github.com/toelt-llc/ADL-Book-2nd-Ed.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzfcWnUuGkVX"
      },
      "source": [
        "# This cell imports some custom written functions that we have created to\n",
        "# make the plotting easier. You don't need\n",
        "# to undertsand the details and you can simply ignore this cell.\n",
        "# Simply run it with CMD+Enter (on Mac) or CTRL+Enter (Windows or Ubuntu) to\n",
        "# import the necessary functions.\n",
        "\n",
        "import sys\n",
        "sys.path.append('ADL-Book-2nd-Ed/modules/')\n",
        "\n",
        "from plot_confusion_matrix import plot_cm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EFOxMEhus07"
      },
      "source": [
        "## MNIST and Fashion MNIST Datasets Import"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9TWvBlH5cqm"
      },
      "source": [
        "For this notebook we will use two datasets:\n",
        "\n",
        "- [MNIST Dataset](http://yann.lecun.com/exdb/mnist/)\n",
        "- [Fashion MNIST @ Zalando](https://research.zalando.com/welcome/mission/research-projects/fashion-mnist/)\n",
        "\n",
        "You can check the two datasets with the links above. They can be easily imported using Keras. Below you can see how easy is using [```tensorflow.keras.datasets```](https://www.tensorflow.org/api_docs/python/tf/keras/datasets)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IQf1yl8uyQ-"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "(mnist_x_train, mnist_y_train), (mnist_x_test, mnist_y_test) = mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUn1Qxoa6LBW"
      },
      "source": [
        "As usual we will do the typical normalisation of the datasets as you can see below. At this point in the book you should be able to understand the code below easily."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MX96nM1u2Z0"
      },
      "source": [
        "mnist_x_train = mnist_x_train.astype('float32') / 255.\n",
        "mnist_x_test = mnist_x_test.astype('float32') / 255.\n",
        "mnist_x_train = mnist_x_train.reshape((len(mnist_x_train), np.prod(mnist_x_train.shape[1:])))\n",
        "mnist_x_test = mnist_x_test.reshape((len(mnist_x_test), np.prod(mnist_x_test.shape[1:])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8TfQEXmvALJ"
      },
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "\n",
        "(fashion_x_train, fashion_y_train), (fashion_x_test, fashion_y_test) = fashion_mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yt0iAVSz6XOA"
      },
      "source": [
        "Note that we are doing the same normalisation for the fashion mnist datasets as for the classical MNIST."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lw3IFaPeu_3s"
      },
      "source": [
        "fashion_x_train = fashion_x_train.astype('float32') / 255.\n",
        "fashion_x_test = fashion_x_test.astype('float32') / 255.\n",
        "fashion_x_train = fashion_x_train.reshape((len(fashion_x_train), np.prod(fashion_x_train.shape[1:])))\n",
        "fashion_x_test = fashion_x_test.reshape((len(fashion_x_test), np.prod(fashion_x_test.shape[1:])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsxM-4FYunrK"
      },
      "source": [
        "## Function to Create the Autoencoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjqQlYFY6dEH"
      },
      "source": [
        "Now we need to create the ```keras``` models. An autoencoder is made of two main parts: an _encoder_ and a _decoder_. The function below ```create_autoencoders()``` returns the following parts as separate models:\n",
        "\n",
        "- the encoder\n",
        "- the decoder\n",
        "- the complete model, when the encoder and decoder are joined in one model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e3_MUYyuHp-"
      },
      "source": [
        "def create_autoencoders(feature_layer_dim = 16):\n",
        "\n",
        "  input_img = Input(shape = (784,), name = 'Input_Layer')\n",
        "  # 784 is the total number of pixels of MNIST images\n",
        "\n",
        "  # The layer encoded has a dimension equal to feature_layer_dim and contains\n",
        "  # the encoded input (therefore the name)\n",
        "  encoded = Dense(feature_layer_dim, activation = 'relu', name = 'Encoded_Features')(input_img)\n",
        "  decoded = Dense(784, activation = 'sigmoid', name = 'Decoded_Input')(encoded)\n",
        "\n",
        "  autoencoder = Model(input_img, decoded)\n",
        "  encoder = Model(input_img, encoded)\n",
        "\n",
        "  encoded_input = Input(shape = (feature_layer_dim,))\n",
        "  decoder = autoencoder.layers[-1]\n",
        "  decoder = Model(encoded_input, decoder(encoded_input))\n",
        "\n",
        "  return autoencoder, encoder, decoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6juVqpJvSQg"
      },
      "source": [
        "## Autoencoder with Layers with $(784,16,784)$ Neurons (MNIST Dataset)\n",
        "\n",
        "As a first step let's create an autoencoder with the layer dimensions of $(784, 16, 784)$.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5BaDqnbuY-I"
      },
      "source": [
        "# 16 is the number of latent features of our autoencoder\n",
        "autoencoder, encoder, decoder = create_autoencoders(16)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A157gXaewJI2"
      },
      "source": [
        "keras.utils.plot_model(autoencoder, show_shapes = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jNv0UMqwvJ9"
      },
      "source": [
        "As for any Keras model we need to compile the model and then fit it to the data. As you can see we don't need any custom code to work with autoencoders. A simple model **definition** $\\rightarrow$ **compile** $\\rightarrow$ **fit** is enough."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFv4sGg4mBkB"
      },
      "source": [
        "autoencoder.compile(optimizer = 'adam', loss = 'binary_crossentropy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxaS4XlLmLI7"
      },
      "source": [
        "history = autoencoder.fit(mnist_x_train, mnist_x_train,\n",
        "                          epochs = 30,\n",
        "                          batch_size = 256,\n",
        "                          shuffle = True,\n",
        "                          validation_data = (mnist_x_test, mnist_x_test),\n",
        "                          verbose = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaRLBur6mM2Y"
      },
      "source": [
        "encoded_imgs = encoder.predict(mnist_x_test)\n",
        "decoded_imgs = decoder.predict(encoded_imgs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFwBP1CdKWLi"
      },
      "source": [
        "### Some Notes about Compression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1V3Z9_X-FPY"
      },
      "source": [
        "Since we have saved both the encoder and the decoder we can generate an encoded version of the images. Note that the data type of the images is ```float32```. Each ```float32``` require 4 bytes. The original data ```mnist_x_test``` has a shape of ```(10000,784)```. That means that the space needed to save this array is\n",
        "\n",
        "$$\n",
        "10000 \\textrm{ images} \\times 784 \\textrm{ pixels}\\times 4 \\textrm{ bytes} = 29 \\textrm{ Mb}\n",
        "$$\n",
        "\n",
        "The encoded images require quite less space. Infact the size needed is\n",
        "\n",
        "$$\n",
        "10000 \\textrm{ images}\\times 16 \\textrm{ latent features}\\times 4 \\textrm{ bytes} = 625 \\textrm{ Kb}\n",
        "$$\n",
        "\n",
        "so if we accept the loss in quality in the compressed data we have achieved an incredible compression ratio of ca. $46$. It is easy to check those sizes by saving the data in a file and checking the file size. Note that the file size will be bigger, since there is some additional information that needs to be saved."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1lYjk66-ZQs"
      },
      "source": [
        "np.save('temp_orig', mnist_x_test)\n",
        "! ls -al temp_orig*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjRKH8t1NBBP"
      },
      "source": [
        "np.save('temp_encoded', encoded_imgs)\n",
        "! ls -al temp_encoded*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZ83T02HNG23"
      },
      "source": [
        "### Reconstructed Images Analysis\n",
        "\n",
        "As noted above we have basically compressed our input images. But at what price? In the images below you can see what is the effect of having only 16 neurons in the middle layer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzqmwmRznzSH"
      },
      "source": [
        "n = 10  # how many digits we will display\n",
        "\n",
        "fig = plt.figure(figsize = (20, 4))\n",
        "\n",
        "for i in range(n):\n",
        "\n",
        "    # display original\n",
        "    ax = plt.subplot(2, n, i + 1)\n",
        "    plt.imshow(mnist_x_test[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # display reconstruction\n",
        "    ax = plt.subplot(2, n, i + 1 + n)\n",
        "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inspection of Latent Space Geometry\n",
        "Questions\n",
        "\n",
        "- Which numbers, in your opinion, have similar shapes\n",
        "- Does the learned encoding reflect this similarity?\n",
        "- In other words, did the autoencoder learn to cluster the digits by shape similarity?"
      ],
      "metadata": {
        "id": "ow_CqcIMt3Ex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For visualization, reduce encodings to two dimensions.\n",
        "# Here, we use principal component analysis (PCA) for this.\n",
        "pca = PCA(n_components=2)\n",
        "pca.fit(encoded_imgs)\n",
        "encoded_imgs_pca = pca.transform(encoded_imgs)\n",
        "\n",
        "fig = plt.figure(figsize=(4, 4))\n",
        "cmap = plt.get_cmap('tab10')\n",
        "plt.scatter(encoded_imgs_pca[:, 0].reshape(-1),\n",
        "            encoded_imgs_pca[:, 1].reshape(-1),\n",
        "            s=1,\n",
        "            c=mnist_y_test, cmap=cmap, vmin=0-0.5, vmax=9+0.5)\n",
        "plt.colorbar(ticks=np.arange(0, 9+1))\n",
        "plt.xlabel('First principal component')\n",
        "plt.ylabel('Second principal component')\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mJDHP2efoVAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interpolation between Codes"
      ],
      "metadata": {
        "id": "IFTbtcpczxug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_start = mnist_x_test[0]\n",
        "img_end = mnist_x_test[3]\n",
        "\n",
        "img_start_encoded = encoder.predict(np.expand_dims(img_start, axis=0))\n",
        "img_end_encoded = encoder.predict(np.expand_dims(img_end, axis=0))\n",
        "\n",
        "blending_factors = np.linspace(0.0, 1.0, 6)\n",
        "\n",
        "fig = plt.figure(figsize=(10, 2))\n",
        "\n",
        "for i, blending_factor in enumerate(blending_factors):\n",
        "    img_blended_encoding = (\n",
        "        (1 - blending_factor) * img_start_encoded\n",
        "        + blending_factor * img_end_encoded\n",
        "    )\n",
        "    img_blended = decoder.predict(img_blended_encoding)[0]\n",
        "\n",
        "    ax = plt.subplot(1, len(blending_factors), i + 1)\n",
        "    plt.imshow(img_blended.reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yThlEHHoz2ne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How robust is the code?"
      ],
      "metadata": {
        "id": "Nz4v_Y0vvbm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img = mnist_x_test[0]\n",
        "img_encoded = encoder.predict(np.expand_dims(img, axis=0))\n",
        "\n",
        "noise = np.random.normal(scale=1, size=img_encoded.shape)\n",
        "noise_factors = np.arange(6, dtype=float)\n",
        "\n",
        "fig = plt.figure(figsize=(10, 2))\n",
        "\n",
        "for i, noise_factor in enumerate(noise_factors):\n",
        "    img_encoded_with_noise = img_encoded + noise_factor * noise\n",
        "    img_decoded_from_noisy_code = decoder.predict(img_encoded_with_noise)\n",
        "\n",
        "    ax = plt.subplot(1, len(noise_factors), i + 1)\n",
        "    plt.imshow(img_decoded_from_noisy_code[0].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.set_title(f\"{noise_factor} x noise\")\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8MKJWruqvZL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7O6b2A0tP8j"
      },
      "source": [
        "## Autoencoder with Layers with $(784,64,784)$ Neurons (MNIST Dataset)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFpLzQ0MtTkH"
      },
      "source": [
        "# now we use 64 latent features\n",
        "autoencoder, encoder, decoder = create_autoencoders(64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cusA73eRtWgW"
      },
      "source": [
        "autoencoder.compile(optimizer = 'adam', loss = 'binary_crossentropy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxSZA8tdtYRj"
      },
      "source": [
        "autoencoder.fit(mnist_x_train, mnist_x_train,\n",
        "                epochs = 30,\n",
        "                batch_size = 256,\n",
        "                shuffle = True,\n",
        "                validation_data = (mnist_x_test, mnist_x_test),\n",
        "                verbose = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzN50nAoteN8"
      },
      "source": [
        "encoded_imgs = encoder.predict(mnist_x_test)\n",
        "decoded_imgs = decoder.predict(encoded_imgs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CW5Afyoltf3M"
      },
      "source": [
        "n = 10  # how many digits we will display\n",
        "\n",
        "fig = plt.figure(figsize=(20, 4))\n",
        "\n",
        "for i in range(n):\n",
        "\n",
        "    # display original\n",
        "    ax = plt.subplot(2, n, i + 1)\n",
        "    plt.imshow(mnist_x_test[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # display reconstruction\n",
        "    ax = plt.subplot(2, n, i + 1 + n)\n",
        "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ow0kHnTzupz"
      },
      "source": [
        "## Autoencoder with Layers with $(784,8,784)$ Neurons (MNIST Dataset)\n",
        "\n",
        "We can check how, reducing the number of neurons in the middle layer, the quality of the reconstruction drops since the dimensionality reduction is too extreme in this case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwGSERoswp7Y"
      },
      "source": [
        "# in this final example, we use only 8 latent features\n",
        "autoencoder, encoder, decoder = create_autoencoders(8)\n",
        "autoencoder.compile(optimizer = 'adam', loss = 'binary_crossentropy')\n",
        "autoencoder.fit(mnist_x_train, mnist_x_train,\n",
        "                epochs = 30,\n",
        "                batch_size = 256,\n",
        "                shuffle = True,\n",
        "                validation_data = (mnist_x_test, mnist_x_test),\n",
        "                verbose = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLglZqluwxX3"
      },
      "source": [
        "encoded_imgs = encoder.predict(mnist_x_test)\n",
        "decoded_imgs = decoder.predict(encoded_imgs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4EdcV8uw0VR"
      },
      "source": [
        "n = 10  # how many digits we will display\n",
        "\n",
        "fig = plt.figure(figsize=(20, 4))\n",
        "\n",
        "for i in range(n):\n",
        "\n",
        "    # display original\n",
        "    ax = plt.subplot(2, n, i + 1)\n",
        "    plt.imshow(mnist_x_test[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # display reconstruction\n",
        "    ax = plt.subplot(2, n, i + 1 + n)\n",
        "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "#fig.savefig('comparison_8.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fD0-hMwzZT1"
      },
      "source": [
        "## Classification with the Latent Features (MNIST Dataset)\n",
        "Now let's check how we can use the latent features, or in other words the output of the middle layer, to do some classification. Are we loosing much accuracy? What about performance of the algorithms? Let's check some examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_1NiEZg_2oD"
      },
      "source": [
        "### KNN Study\n",
        "\n",
        "The first algorithm that we can test is KNN. To do it we can simply use ```sklearn```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Hu9WJ_b4CjW"
      },
      "source": [
        "encoded_train_imgs = encoder.predict(mnist_x_train)\n",
        "# decoded_imgs = decoder.predict(encoded_imgs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RL98uuEV2dqg"
      },
      "source": [
        "Note how the variable ```encoded_train_imgs``` has only 8 features (remember the last autoencoders we trained had only 8 neurons in the middle layer). The original dataset had 784 features (the pixel gray values of the images)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMy5q_5F2h7b"
      },
      "source": [
        "encoded_train_imgs.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlagb73X2xm4"
      },
      "source": [
        "We will now train the classifier with the encoded images and we will measure how long it takes for the training. Later we will do the same with the original dataset and compare results and running time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEZuHJg8ff7j"
      },
      "source": [
        "start = time.time()\n",
        "\n",
        "# training a KNN classifier\n",
        "knn = KNeighborsClassifier(n_neighbors = 7).fit(encoded_train_imgs, mnist_y_train)\n",
        "\n",
        "# accuracy on X_test\n",
        "accuracy = knn.score(encoded_imgs, mnist_y_test)\n",
        "print('The accuracy on the test set is: ', accuracy)\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "print('Running time', end - start, 'sec.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huxTT5p03CQ1"
      },
      "source": [
        "With just the 8 features we get an incredible 90% of accuracy in 1.2 sec."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cef3h6oHJ6T"
      },
      "source": [
        "# The following line contains the class with the method that enables confusion matrix plotting.\n",
        "pcm = plot_cm()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJXa2Oye40K_"
      },
      "source": [
        "# creating a confusion matrix\n",
        "knn_predictions = knn.predict(encoded_imgs)\n",
        "cm = confusion_matrix(mnist_y_test, knn_predictions)\n",
        "pcm.plot_confmat(cm, [0,1,2,3,4,5,6,7,8,9], 'confusion_matrix.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUySdOkeaA_X"
      },
      "source": [
        "### KNN with All the Features\n",
        "\n",
        "Now let's train a classifier with all 784 features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXhGvgfpaDie"
      },
      "source": [
        "start = time.time()\n",
        "\n",
        "knn2 = KNeighborsClassifier(n_neighbors = 7).fit(mnist_x_train, mnist_y_train)\n",
        "\n",
        "# accuracy on X_test\n",
        "accuracy = knn2.score(mnist_x_test, mnist_y_test)\n",
        "print('The accuracy on the test set is: ', accuracy)\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "print('Running time', end - start, 'sec.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-c0ZVIw3TfH"
      },
      "source": [
        "With all the features we get 97% accuracy (7% more than with just 8 features) but it takes 1000 times longer, around 16 minutes. If instead of 10000 observations we had few millions, this 1000 time gain will become not only significant but it may mean the difference between training a model or not being able to."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVvL5CtKooYi"
      },
      "source": [
        "## MSE (MNIST Dataset)\n",
        "\n",
        "Autoencoders are so flexible that they work even if we use as loss function the MSE. We can try it quickly in the example below. More details about the loss functions are discussed in the book."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHeZ-jh-pIHN"
      },
      "source": [
        "dim = 16\n",
        "\n",
        "input_img = Input(shape = (784,))\n",
        "encoded = Dense(dim, activation = 'relu')(input_img)\n",
        "decoded = Dense(784, activation = 'sigmoid')(encoded)\n",
        "\n",
        "autoencoder = Model(input_img, decoded)\n",
        "encoder = Model(input_img, encoded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yne4LJ5Qprml"
      },
      "source": [
        "encoded_input = Input(shape = (dim,))\n",
        "decoder = autoencoder.layers[-1]\n",
        "decoder = Model(encoded_input, decoder(encoded_input))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x25we9gypuQ8"
      },
      "source": [
        "autoencoder.compile(optimizer = 'adam', loss = 'mse')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Wv5Cb89pwU7"
      },
      "source": [
        "autoencoder.fit(mnist_x_train, mnist_x_train,\n",
        "                epochs = 30,\n",
        "                batch_size = 256,\n",
        "                shuffle = True,\n",
        "                validation_data = (mnist_x_test, mnist_x_test),\n",
        "                verbose = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWvGMfR5pzBq"
      },
      "source": [
        "encoded_imgs = encoder.predict(mnist_x_test)\n",
        "decoded_imgs = decoder.predict(encoded_imgs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGuhZFYcp2Pc"
      },
      "source": [
        "n = 10  # how many digits we will display\n",
        "\n",
        "fig = plt.figure(figsize=(20, 4))\n",
        "\n",
        "for i in range(n):\n",
        "\n",
        "    # display original\n",
        "    ax = plt.subplot(2, n, i + 1)\n",
        "    plt.imshow(mnist_x_test[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # display reconstruction\n",
        "    ax = plt.subplot(2, n, i + 1 + n)\n",
        "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "#fig.savefig('comparison_MSE_16.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBAq_uiwp3wt"
      },
      "source": [
        "## Fashion MNIST\n",
        "\n",
        "We can use an autoencoder with the FASHION MNIST dataset now. We will just try to build one with 8 neurons in the middle layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CH-E8OZ-s44c"
      },
      "source": [
        "autoencoder, encoder, decoder = create_autoencoders(8)\n",
        "autoencoder.compile(optimizer = 'adam', loss = 'binary_crossentropy')\n",
        "autoencoder.fit(fashion_x_train, fashion_x_train,\n",
        "                epochs = 30,\n",
        "                batch_size = 256,\n",
        "                shuffle = True,\n",
        "                validation_data = (fashion_x_test, fashion_x_test),\n",
        "                verbose = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcChJ6NCtYr1"
      },
      "source": [
        "encoded_imgs = encoder.predict(fashion_x_test)\n",
        "decoded_imgs = decoder.predict(encoded_imgs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3WdZxsitKgk"
      },
      "source": [
        "n = 10  # how many digits we will display\n",
        "\n",
        "plt.figure(figsize = (20, 4))\n",
        "\n",
        "for i in range(n):\n",
        "\n",
        "    # display original\n",
        "    ax = plt.subplot(2, n, i + 1)\n",
        "    plt.imshow(fashion_x_test[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # display reconstruction\n",
        "    ax = plt.subplot(2, n, i + 1 + n)\n",
        "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SGHRd1tCobZ"
      },
      "source": [
        "And again we can try to build a classifier with all the data. Also in this case, as it would be expected, it take almost 16 minutes. Quite a long time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1nP0LVDtT-3"
      },
      "source": [
        "start = time.time()\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors = 7).fit(fashion_x_train, fashion_y_train)\n",
        "\n",
        "# accuracy on X_test\n",
        "accuracy = knn.score(fashion_x_test, fashion_y_test)\n",
        "print('The accuracy on the test set is: ', accuracy)\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "print('Running time', end - start, 'sec.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7z1rIeQ-iEJT"
      },
      "source": [
        "### KNN on Learned Representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YL9BbtyRh23b"
      },
      "source": [
        "encoded_fashion_train_imgs = encoder.predict(fashion_x_train)\n",
        "encoded_fashion_test_imgs = encoder.predict(fashion_x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FS2Q4_AKiL5-"
      },
      "source": [
        "start = time.time()\n",
        "\n",
        "# training a KNN classifier\n",
        "knn = KNeighborsClassifier(n_neighbors = 7).fit(encoded_fashion_train_imgs, fashion_y_train)\n",
        "\n",
        "# accuracy on X_test\n",
        "accuracy = knn.score(encoded_fashion_test_imgs, fashion_y_test)\n",
        "print('The accuracy on the test set is: ', accuracy)\n",
        "\n",
        "end = time.time()\n",
        "print('Running time', end - start, 'sec.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdEdeWCLEt-O"
      },
      "source": [
        "So again we loose ca. 5% of accuracy but we are 1000 times faster. Quite impressive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EopH1TOuiX7J"
      },
      "source": [
        "### KNN Accuracy with an Autoencoder with Number of Neurons in the Layers $(784,16,784)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PPfUo28mIgX"
      },
      "source": [
        "autoencoder, encoder, decoder = create_autoencoders (16)\n",
        "autoencoder.compile(optimizer = 'adam', loss = 'binary_crossentropy')\n",
        "autoencoder.fit(fashion_x_train, fashion_x_train,\n",
        "                epochs = 30,\n",
        "                batch_size = 256,\n",
        "                shuffle = True,\n",
        "                validation_data = (fashion_x_test, fashion_x_test),\n",
        "                verbose = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuAhbpAQmJqN"
      },
      "source": [
        "encoded_fashion_train_imgs = encoder.predict(fashion_x_train)\n",
        "encoded_fashion_test_imgs = encoder.predict(fashion_x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44ogzPh1mO9l"
      },
      "source": [
        "start = time.time()\n",
        "\n",
        "# training a KNN classifier\n",
        "knn = KNeighborsClassifier(n_neighbors = 7).fit(encoded_fashion_train_imgs, fashion_y_train)\n",
        "\n",
        "# accuracy on X_test\n",
        "accuracy = knn.score(encoded_fashion_test_imgs, fashion_y_test)\n",
        "print('The accuracy on the test set is: ', accuracy)\n",
        "\n",
        "end = time.time()\n",
        "print('Running time', end - start, 'sec.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "It2Kf7opE7Pm"
      },
      "source": [
        "Now increasing the number of neurons in the middle layer to 16 will give us an accuracy equal to 84% in 3 seconds. A minor increase in running time that is well worth! Still doing the same classification with all the features would require 1000 sec."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfOxC86D6b8G"
      },
      "source": [
        "## Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExgkY36I6esu"
      },
      "source": [
        "1. [*Easy Difficulty*] Try to build yourself three different autoencoders with the following layers: $(784,64,784)$, $(784,16,784)$ and $(784,8,784)$ and apply them to the FASHION MNIST dataset. Then, plot together the original version of the images and the encoded ones. See how results change in each case and decide which is the best trade-off between images' resolution and size.\n",
        "2. [*Medium Difficulty*] Try the same autoencoders you developed in Exercise 1, adding one hidden layer of 300 units inside the encoder and another identical one inside the decoder. See how plots change, always comparing results with the original images.\n",
        "3. [*Medium Difficulty*] Try the notebook on another online dataset, for example this: https://www.kaggle.com/andrewmvd/animal-faces (containing 16130 high-quality images of cats, dogs and wildlife animals at 512×512 resolution).\n",
        "4. [*Hard Difficulty*] Using the autoencoder you developed in Exercise 2 (the one with two hidden layers) try to visualize the features learned by the autoencoder itself, in the following way: for each neuron in the first hidden layer create an image where a pixel's intensity corresponds to the weight of the connection to the neuron itself."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1899cKxa6m1F"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}